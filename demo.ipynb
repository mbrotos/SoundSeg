{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mbrotos/SoundSeg/blob/main/demo.ipynb)"]},{"cell_type":"markdown","metadata":{},"source":["# Singing Voice Separation by U-Net"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1702392495777,"user":{"displayName":"Adam Sorrenti","userId":"08015701831222800572"},"user_tz":300},"id":"2PuRNYOqfrNi","outputId":"cc5347ff-190d-4b9b-f3b0-0d063501661f"},"outputs":[],"source":["if 'google.colab' in str(get_ipython()):\n","    print('Running on CoLab, cloning repo, and installing requirements ...')\n","    !git clone https://github.com/mbrotos/SoundSeg.git\n","    %cd SoundSeg/src\n","    !pip install -r requirements.txt > /dev/null\n","    !nvidia-smi\n","    print('Make sure to enable GPU acceleration in CoLab by going to Edit > Notebook Settings > Hardware Accelerator')\n","else:\n","    print('Not running on CoLab, creating local environment and installing requirements ...')\n","    %cd src\n","    !python -m venv venv\n","    !source ./venv/bin/activate\n","    !pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10817,"status":"ok","timestamp":1702392506591,"user":{"displayName":"Adam Sorrenti","userId":"08015701831222800572"},"user_tz":300},"id":"8OI3EtugfrNj"},"outputs":[],"source":["import os\n","import librosa\n","import IPython.display as ipd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import argparse\n","import config as cfg\n","import datetime\n","from scaler import normalize, denormalize\n","import pickle\n","import json\n","from augmentations import consecutive_oversample, blackout\n","from uuid import uuid4\n","import mir_eval"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2107,"status":"ok","timestamp":1702392652696,"user":{"displayName":"Adam Sorrenti","userId":"08015701831222800572"},"user_tz":300},"id":"AM5uP9l-f42t","outputId":"b43a9912-d233-4678-e94d-832bf1573cdd"},"outputs":[],"source":["# Obtain dataset from Zenodo\n","# MUSDB18 - a corpus for music separation https://doi.org/10.5281/zenodo.3338373\n","# NOTE: This dataset is quite large (>20GB -- zipped) and may take a while to download.\n","\n","# Alternatively, you can download a subset of the dataset (~9GB) with the following command: \n","# https://drive.google.com/file/d/1_kdifA4ztVXBveb9FYzmY49fvAKZmIJF/view?usp=sharing\n","!gdown 1_kdifA4ztVXBveb9FYzmY49fvAKZmIJF && unzip data_wav.zip && rm data_wav.zip\n","\n","\n","\"\"\"\n","# If you are running on Google Colab, you can mount your Google Drive and save the dataset there.\n","# This helps avoid having to download the dataset every time you run the notebook.\n","# Replace \"School/EE8223/Code\" with the path to your folder on Google Drive.\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!cp /content/drive/MyDrive/School/EE8223/Code/data_wav.zip . && unzip data_wav.zip && rm data_wav.zip\n","\"\"\"\n","\n","# This cell may take 5 minutes to run. Please be patient."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create required directories\n","!mkdir -p models processed_data\n","\n","# Preprocess dataset\n","!python preprocessing.py --dsType train\n","!python preprocessing.py --dsType test\n","\n","# Tensorflow dataset prep\n","!python dataset_prep.py\n","\n","# See ./src/preprocessing.py for more details on the preprocessing steps.\n","# See ./src/dataset_prep.py for more details on the dataset preparation steps.\n","\n","\"\"\"Some of these details will be shown in the Evaluate the model section below.\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["#### Song mixutre and vocal example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"536YBpl_iYZS"},"outputs":[],"source":["mix_mags_train = np.load(\"./processed_data/mix_mags_train_512x128.npy\", mmap_mode='r' )\n","mix_phases_train = np.load(\"./processed_data/mix_phases_train_512x128.npy\", mmap_mode='r')\n","vocal_train = np.load(f\"./processed_data/vocal_mags_train_512x128.npy\",mmap_mode='r')\n","print(mix_mags_train.shape, mix_phases_train.shape, vocal_train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mix_wav = librosa.istft(mix_mags_train[0,:,:,0] * mix_phases_train[0,:,:,0], hop_length=cfg.HOP_SIZE)\n","vocal_wav = librosa.istft(vocal_train[0,:,:,0] * mix_phases_train[0,:,:,0], hop_length=cfg.HOP_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Example of a song segment mixture used for training:\")\n","ipd.Audio(mix_wav, rate=cfg.SR)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Example of a song segment vocal used for training:\")\n","ipd.Audio(vocal_wav, rate=cfg.SR)"]},{"cell_type":"markdown","metadata":{},"source":["#### Visualize the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQoPH_iQfrNl"},"outputs":[],"source":["def plot_spectrogram_with_shading(spectrogram, title, subplot_index, shade_start=None, shade_end=None):\n","    plt.subplot(subplot_index)\n","    plt.imshow(spectrogram, aspect='auto', origin='lower')\n","    if shade_start is not None and shade_end is not None:\n","        plt.axvspan(shade_start, shade_end, color='red', alpha=0.2)\n","    plt.title(title)\n","    plt.colorbar()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(18, 6))\n","plot_spectrogram_with_shading(mix_mags_train[0,:,:,0], \"Mixture Magnitude\", 131)\n","plot_spectrogram_with_shading(vocal_train[0,:,:,0], \"Vocal Magnitude\", 132)"]},{"cell_type":"markdown","metadata":{},"source":["### Run unit tests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ex7NzjQZfrNl"},"outputs":[],"source":["!python test_audio_processing.py\n","# See ./src/test_audio_processing.py for more details on the audio processing unit tests I created for this project."]},{"cell_type":"markdown","metadata":{},"source":["### Define Model Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0DgJpmGfrNm"},"outputs":[],"source":["import tensorflow as tf\n","from keras.layers import Activation, Conv2D, BatchNormalization, Conv2DTranspose, Concatenate, MaxPooling2D, Input, Conv1D, Normalization\n","\n","def get_model(img_size, num_classes=1):\n","    inputs = Input(shape=img_size + (1,))\n","\n","    conv1 = Conv2D(64, 3, strides=1, padding=\"same\")(inputs)\n","    conv1 = BatchNormalization()(conv1)\n","    conv1 = Activation(\"relu\")(conv1)\n","\n","    conv2 = Conv2D(64, 3, strides=1, padding=\"same\")(conv1)\n","    conv2 = BatchNormalization()(conv2)\n","    conv2 = Activation(\"relu\")(conv2)\n","\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","\n","    conv3 = Conv2D(128, 3, strides=1, padding=\"same\")(pool1)\n","    conv3 = BatchNormalization()(conv3)\n","    conv3 = Activation(\"relu\")(conv3)\n","\n","    conv4 = Conv2D(128, 3, strides=1, padding=\"same\")(conv3)\n","    conv4 = BatchNormalization()(conv4)\n","    conv4 = Activation(\"relu\")(conv4)\n","\n","    pool2 = MaxPooling2D(pool_size=(2, 2))(conv4)\n","\n","    conv5 = Conv2D(256, 3, strides=1, padding=\"same\")(pool2)\n","    conv5 = BatchNormalization()(conv5)\n","    conv5 = Activation(\"relu\")(conv5)\n","\n","    conv6 = Conv2D(256, 3, strides=1, padding=\"same\")(conv5)\n","    conv6 = BatchNormalization()(conv6)\n","    conv6 = Activation(\"relu\")(conv6)\n","\n","    pool3 = MaxPooling2D(pool_size=(2, 2))(conv6)\n","\n","    conv7 = Conv2D(512, 3, strides=1, padding=\"same\")(pool3)\n","    conv7 = BatchNormalization()(conv7)\n","    conv7 = Activation(\"relu\")(conv7)\n","\n","    conv8 = Conv2D(512, 3, strides=1, padding=\"same\")(conv7)\n","    conv8 = BatchNormalization()(conv8)\n","    conv8 = Activation(\"relu\")(conv8)\n","\n","    pool4 = MaxPooling2D(pool_size=(2, 2))(conv8)\n","\n","    conv9 = Conv2D(1024, 3, strides=1, padding=\"same\")(pool4)\n","    conv9 = BatchNormalization()(conv9)\n","    conv9 = Activation(\"relu\")(conv9)\n","\n","    conv10 = Conv2D(1024, 3, strides=1, padding=\"same\")(conv9)\n","    conv10 = BatchNormalization()(conv10)\n","    conv10 = Activation(\"relu\")(conv10)\n","\n","    up1 = Conv2DTranspose(512, 2, strides=2, padding=\"same\")(conv10)\n","    up1 = Concatenate()([up1, conv8])\n","\n","    upconv1 = Conv2D(512, 3, strides=1, padding=\"same\")(up1)\n","    upconv1 = BatchNormalization()(upconv1)\n","    upconv1 = Activation(\"relu\")(upconv1)\n","\n","    upconv2 = Conv2D(512, 3, strides=1, padding=\"same\")(upconv1)\n","    upconv2 = BatchNormalization()(upconv2)\n","    upconv2 = Activation(\"relu\")(upconv2)\n","\n","    up2 = Conv2DTranspose(256, 2, strides=2, padding=\"same\")(upconv2)\n","    up2 = Concatenate()([up2, conv6])\n","\n","    upconv3 = Conv2D(256, 3, strides=1, padding=\"same\")(up2)\n","    upconv3 = BatchNormalization()(upconv3)\n","    upconv3 = Activation(\"relu\")(upconv3)\n","\n","    upconv4 = Conv2D(256, 3, strides=1, padding=\"same\")(upconv3)\n","    upconv4 = BatchNormalization()(upconv4)\n","    upconv4 = Activation(\"relu\")(upconv4)\n","\n","    up3 = Conv2DTranspose(128, 2, strides=2, padding=\"same\")(upconv4)\n","    up3 = Concatenate()([up3, conv4])\n","\n","    upconv5 = Conv2D(128, 3, strides=1, padding=\"same\")(up3)\n","    upconv5 = BatchNormalization()(upconv5)\n","    upconv5 = Activation(\"relu\")(upconv5)\n","\n","    upconv6 = Conv2D(128, 3, strides=1, padding=\"same\")(upconv5)\n","    upconv6 = BatchNormalization()(upconv6)\n","    upconv6 = Activation(\"relu\")(upconv6)\n","\n","    up4 = Conv2DTranspose(64, 2, strides=2, padding=\"same\")(upconv6)\n","    up4 = Concatenate()([up4, conv2])\n","\n","    upconv7 = Conv2D(64, 3, strides=1, padding=\"same\")(up4)\n","    upconv7 = BatchNormalization()(upconv7)\n","    upconv7 = Activation(\"relu\")(upconv7)\n","\n","    upconv8 = Conv2D(64, 3, strides=1, padding=\"same\")(upconv7)\n","    upconv8 = BatchNormalization()(upconv8)\n","    upconv8 = Activation(\"relu\")(upconv8)\n","\n","    output = Conv1D(num_classes, 1, activation=\"linear\")(upconv8)\n","\n","    # Define the model\n","    model = tf.keras.Model(inputs, output)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--epochs\", type=int, default=10, help='Number of epochs to train for')\n","parser.add_argument(\"--batch_size\", type=int, default=5, help='Batch size for training')\n","parser.add_argument(\"--normalization\", type=str, default=\"frequency\", help='Normalization axis (time or frequency)')\n","parser.add_argument(\"--lr\", type=float, default=1e-3, help='Learning rate for training')\n","parser.add_argument(\"--mask\", action=\"store_true\", default=False, help='Experimental. Causes unstable training.')\n","parser.add_argument(\"--quantile_scaler\", action=\"store_true\", default=False, help='Toggle quantile scaling as the normalization method')\n","parser.add_argument(\"--q_min\", type=float, default=25.0, help='Minimum quantile for quantile scaling')\n","parser.add_argument(\"--q_max\", type=float, default=75.0, help='Maximum quantile for quantile scaling')\n","parser.add_argument(\"--loss\", type=str, default=\"mse\", help='Loss function to use (mse or mae)')\n","parser.add_argument(\"--dataset_size\", type=int, default=None, help='Number of samples to use from the dataset (None = all)')\n","parser.add_argument(\"--augmentations\", action=\"store_true\", default=False, help='Toggle data augmentations (splicing, and blackout)')\n","parser.add_argument(\"--seed\", type=int, default=42, help='Random seed for reproducibility')\n","parser.add_argument(\"--mmap\", action=\"store_true\", default=True, help='Toggle memory mapping for dataset loading (helps with large datasets and limited RAM)')\n","\n","# Top performaning model args, the batch size has been reduced from 64 to 15 to fit on colab GPUs\n","# The epochs were also reduced from 20 to 5 to save time\n","args = parser.parse_args(['--normalization', 'frequency', '--epochs', '5', '--batch_size', '15', '--loss', 'mae', '--augmentations'])\n","\n","print('Args:')\n","print(args)\n","\n","np.random.seed(args.seed)\n","tf.random.set_seed(args.seed)"]},{"cell_type":"markdown","metadata":{},"source":["### Define datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tliCGkG0frNm"},"outputs":[],"source":["# load data\n","mix_mags_train = np.load(\"./processed_data/mix_mags_train_512x128.npy\", mmap_mode='r' if args.mmap else None)[:args.dataset_size]\n","mix_phases_train = np.load(\"./processed_data/mix_phases_train_512x128.npy\", mmap_mode='r' if args.mmap else None)[:args.dataset_size]\n","vocal_train = np.load( f\"./processed_data/vocal_mags_train_512x128.npy\", mmap_mode='r' if args.mmap else None)[:args.dataset_size]\n","\n","mix_mags_train_norm, vocal_train_norm, mix_mags_train_norm_factors = normalize(\n","    np.copy(mix_mags_train),\n","    np.copy(vocal_train),\n","    normalization=args.normalization,\n","    quantile_scaler=args.quantile_scaler,\n","    q_min=args.q_min,\n","    q_max=args.q_max,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if args.augmentations:\n","    print('Appling augmentations...')\n","    \n","    # Remove outliers\n","    true_vocal = denormalize(\n","        vocal_train_norm,\n","        mix_mags_train_norm_factors,\n","        normalization=args.normalization,\n","        quantile_scaler=args.quantile_scaler,\n","    )\n","\n","    vocal_waves = []\n","\n","    for i in range(0, len(true_vocal)):\n","        cur_phase = np.concatenate(mix_phases_train[i : i + 1], axis=1)\n","        cur_true_vocal = np.concatenate(true_vocal[i : i + 1], axis=1)\n","        vocal_waves.append(librosa.istft(\n","                cur_true_vocal[:, :, 0] * cur_phase[:, :, 0],\n","                hop_length=cfg.HOP_SIZE,\n","                window=\"hann\",\n","            )\n","        )\n","    vocal_waves = np.array(vocal_waves)\n","    dist = np.abs(vocal_waves).sum(axis=1)\n","    indices = np.where(dist < 100)[0]\n","\n","    mix_mags_train_norm = np.delete(mix_mags_train_norm, indices, axis=0)\n","    vocal_train_norm = np.delete(vocal_train_norm, indices, axis=0)\n","    mix_mags_train_norm_factors = np.delete(mix_mags_train_norm_factors, indices, axis=0)\n","    mix_phases_train = np.delete(mix_phases_train, indices, axis=0)\n","    \n","    # Splicing and blackout\n","    mix_blackout, vocal_blackout = blackout(mix_mags_train_norm, vocal_train_norm)\n","    mix_blackout = mix_blackout[:mix_blackout.shape[0]//4]\n","    vocal_blackout = vocal_blackout[:vocal_blackout.shape[0]//4]\n","    mix_consec, vocal_consec = consecutive_oversample(mix_mags_train_norm, vocal_train_norm)\n","    mix_consec = mix_consec[:mix_consec.shape[0]//2]\n","    vocal_consec = vocal_consec[:vocal_consec.shape[0]//2]\n","\n","    mix_mags_train_norm = np.concatenate((mix_mags_train_norm, mix_consec, mix_blackout), axis=0)\n","    vocal_train_norm = np.concatenate((vocal_train_norm, vocal_consec, vocal_blackout), axis=0)\n","    \n","    \n","    \n","    \n","print('Datasets:')\n","print(f'Mixes: {mix_mags_train_norm.shape}')\n","print(f'Vocals: {vocal_train_norm.shape}')"]},{"cell_type":"markdown","metadata":{},"source":["#### Augmentation Visualizations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["i=2\n","mix_blackout_test, vocal_blackout_test = blackout(mix_mags_train_norm[i:i+1], vocal_train_norm[i:i+1])\n","print('Notice the blackout regions in the spectrograms below.')\n","# Plotting\n","plt.figure(figsize=(12, 8))\n","\n","# Original Mix Spectrogram\n","plot_spectrogram_with_shading(mix_mags_train_norm[i], 'Original Mix', 321)\n","\n","# Original Vocal Spectrogram\n","plot_spectrogram_with_shading(vocal_train_norm[i], 'Original Vocal', 322)\n","\n","# Blackout Mix Spectrogram\n","plot_spectrogram_with_shading(mix_blackout_test[0], 'Blackout Mix', 323)\n","\n","# Blackout Vocal Spectrogram\n","plot_spectrogram_with_shading(vocal_blackout_test[0], 'Blackout Vocal', 324)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('Notice the Consecutive Oversample Mix is a mix of the original mix and the mix with 1 sample shifted over.')\n","mix_consec_test, _ = consecutive_oversample(mix_mags_train_norm[i:i+2], vocal_train_norm[i:i+1])\n","# Plotting Consecutive Oversample with shaded highlights\n","plt.figure(figsize=(18, 6))\n","\n","# Original Mix Spectrogram with shading on the last half\n","plot_spectrogram_with_shading(mix_mags_train_norm[i], 'Original Mix', 131, shade_start=64, shade_end=128)\n","\n","# Original Mix + 1 Spectrogram with shading on the first half\n","plot_spectrogram_with_shading(mix_mags_train_norm[i+1], 'Original Mix + 1', 132, shade_start=0, shade_end=64)\n","\n","# Consecutive Oversample Mix Spectrogram with corresponding shaded highlights\n","plot_spectrogram_with_shading(mix_consec_test[0], 'Consecutive Oversample Mix', 133)\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","model_name = f\"model_{timestamp}_{uuid4().hex}\"\n","    \n","# Save normalization factors as pkl\n","with open(f\"./models/scaler--{model_name}.pkl\", \"wb\") as f:\n","    pickle.dump(mix_mags_train_norm_factors, f)\n","\n","data_len = len(mix_mags_train_norm)\n","\n","# shuffle datasets before splitting\n","indices = np.arange(data_len)\n","np.random.shuffle(indices)\n","mix_mags_train_norm = mix_mags_train_norm[indices]\n","vocal_train_norm = vocal_train_norm[indices]\n","\n","val_len = int(data_len * 0.1)\n","val_data = (\n","    mix_mags_train_norm[-val_len:],\n","    vocal_train_norm[-val_len:],\n",")\n","train_data = (\n","    mix_mags_train_norm[:-val_len],\n","    vocal_train_norm[:-val_len],\n",")\n","\n","dataset = tf.data.Dataset.from_tensor_slices(train_data)\n","dataset = (\n","    dataset.shuffle(args.batch_size * 2)\n","    .batch(args.batch_size)\n","    .prefetch(tf.data.AUTOTUNE)\n",")\n","val_ds = tf.data.Dataset.from_tensor_slices(val_data)\n","val_ds = val_ds.batch(args.batch_size).prefetch(tf.data.AUTOTUNE)"]},{"cell_type":"markdown","metadata":{},"source":["### Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = get_model((cfg.FREQUENCY_BINS, cfg.SAMPLE_SZ), num_classes=1)\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(args.lr),\n","    loss=args.loss,\n",")\n","\n","os.makedirs(f\"./models/{model_name}\")\n","os.makedirs(f\"./models/{model_name}/logs\")\n","\n","# Save args as json\n","with open(f\"./models/{model_name}/args.json\", \"w\") as f:\n","    json.dump(vars(args), f, indent=4)\n","    \n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwOW4PaRfrNm"},"outputs":[],"source":["history = model.fit(\n","    dataset,\n","    validation_data=val_ds,\n","    epochs=args.epochs,\n","    callbacks=[\n","        tf.keras.callbacks.ModelCheckpoint(\n","            filepath=f\"./models/{model_name}/{model_name}-val.hdf5\", save_best_only=True, monitor='val_loss', mode='min', save_weights_only=True\n","        ),\n","        tf.keras.callbacks.ModelCheckpoint(\n","            filepath=f\"./models/{model_name}/{model_name}-train.hdf5\", save_best_only=True, monitor='loss', mode='min', save_weights_only=True\n","        ),\n","        tf.keras.callbacks.TensorBoard(\n","            log_dir=f\"./models/{model_name}/logs\", histogram_freq=1\n","        ),\n","    ],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","# Extracting the number of epochs\n","epochs = range(1, len(loss) + 1)\n","\n","# Plotting the loss and validation loss\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs, loss, 'bo-', label='Training loss')\n","plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluate the model"]},{"cell_type":"markdown","metadata":{},"source":["#### Test voice seperation"]},{"cell_type":"markdown","metadata":{},"source":["##### Download and extract a song"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9NHtFDifrNn"},"outputs":[],"source":["# This requires ffmpeg to be installed on your system\n","\n","\n","# Replace url with the song you want to test\n","url = 'https://www.youtube.com/watch?v=TLV4_xaYynY' # \"All Along The Watchtower\" by The Jimi Hendrix Experience\n","!yt-dlp {url} -f mp4 -o ./test_audio.mp4 && ffmpeg -i ./test_audio.mp4 -ac 1 -ar {cfg.SR} ./test_audio.wav -y\n","\n","song = librosa.load('./test_audio.wav', sr=cfg.SR, mono=True)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Trim song and test song audio\n","start_time_sec = 18\n","end_time_sec = 53\n","start_time = start_time_sec * cfg.SR\n","end_time = end_time_sec * cfg.SR\n","song = song[start_time:end_time]\n","print(\"Song waveform mixture:\")\n","ipd.Audio(song, rate=cfg.SR)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract spectrogram from waveform\n","\n","mix_stft = librosa.stft(song, n_fft=cfg.FRAME_SIZE, hop_length=cfg.HOP_SIZE, window='hann')\n","mix_mag, mix_phase = librosa.magphase(mix_stft)\n","\n","numOfSamples = mix_mag.shape[1] // cfg.SAMPLE_SZ # This will cut off the last bit of the song\n","        \n","# Print some info\n","print(f\"Number of samples: {numOfSamples}\")\n","print(f\"Shape of mix_mag: {mix_mag.shape}\")\n","    \n","mix_mag_samples = np.array(np.split(mix_mag[:512,:cfg.SAMPLE_SZ*numOfSamples], numOfSamples, axis=1))[:,:,:,np.newaxis]\n","# Trim phase information to match the shape of the magnitude\n","mix_phase = mix_phase[:512,:numOfSamples*cfg.SAMPLE_SZ]\n","mix_phase_samples = np.array(np.split(mix_phase, numOfSamples, axis=1))[:,:,:,np.newaxis]\n","\n","# Print some info\n","print(f\"Shape of mix_mag_samples: {mix_mag_samples.shape}\")\n","print(f\"Shape of mix_phase_samples: {mix_phase_samples.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example of a spectrogram\n","plot_spectrogram_with_shading(mix_mag_samples[3], 'Mix Spectrogram', 111)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Normalize mix_mag_samples\n","youtube_x, _, youtube_x_norm_factors = normalize(\n","        np.copy(mix_mag_samples),\n","        np.copy(mix_mag_samples),\n","        normalization=args.normalization,\n","        quantile_scaler=args.quantile_scaler,\n","        q_min=args.q_min,\n","        q_max=args.q_max,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def denormalize_and_istft(pred_norm, norm_factors, phase):\n","    pred = denormalize(\n","        pred_norm,\n","        norm_factors,\n","        normalization=args.normalization,\n","        quantile_scaler=args.quantile_scaler,\n","    )\n","    pred_comb = np.concatenate(pred, axis=1)\n","    phase_comb = np.concatenate(phase, axis=1)\n","    return librosa.istft(\n","                pred_comb[:, :, 0] * phase_comb[:, :, 0],\n","                hop_length=cfg.HOP_SIZE,\n","                window=\"hann\",\n","            )"]},{"cell_type":"markdown","metadata":{},"source":["##### Predict the vocals from current model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_norm_cur_model = model.predict(youtube_x)\n","pred_wave_cur_model = denormalize_and_istft(pred_norm_cur_model, youtube_x_norm_factors, mix_phase_samples)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Predicted vocal waveform from the current model:\")\n","ipd.Audio(pred_wave_cur_model, rate=cfg.SR) # Not bad! But we can do better."]},{"cell_type":"markdown","metadata":{},"source":["##### Predict the vocals from our best model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download and unzip the pretrained model\n","# Link: https://drive.google.com/file/d/1_n6yMqqxSdr2f_WhLhPwAtC2CSadIH7a/view?usp=sharing\n","\n","best_model_name = 'model_20231208-002503_5244b97903ec49b58783dd64f9c9f5ca'\n","!gdown 1_n6yMqqxSdr2f_WhLhPwAtC2CSadIH7a && unzip {best_model_name}.zip && rm {best_model_name}.zip"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_model = get_model((cfg.FREQUENCY_BINS, cfg.SAMPLE_SZ), num_classes=1)\n","best_model.load_weights(f\"./{best_model_name}/{best_model_name}-val.hdf5\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_norm_best_model = best_model.predict(youtube_x)\n","pred_wave_best_model = denormalize_and_istft(pred_norm_best_model, youtube_x_norm_factors, mix_phase_samples)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Predicted vocal waveform from the best model:\")\n","ipd.Audio(pred_wave_best_model, rate=cfg.SR) \n","# Notice the song is slightly shorter than the original, this is due to the way we split the song into samples"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Original mixed waveform:\")\n","ipd.Audio(song, rate=cfg.SR)"]},{"cell_type":"markdown","metadata":{},"source":["### Comparing data normalization techniques"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"98DcsBTcfrNn"},"outputs":[],"source":["def plot_matrix(matrix, title, ax, decimals=2):\n","    cax = ax.matshow(matrix, cmap=plt.cm.viridis)\n","    plt.colorbar(cax, ax=ax)\n","    ax.set_title(title)\n","    # Time and frequency axis labels\n","    ax.set_xlabel('Time')\n","    ax.set_ylabel('Frequency')\n","    for (i, j), val in np.ndenumerate(matrix):\n","        ax.text(j, i, f\"{val:.{decimals}f}\", ha='center', va='center', color='white')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's recreate the small matrix using only integer values for the unnormalized spectrogram.\n","np.random.seed(0)\n","small_spectrogram = np.random.randint(0, 100, (10, 5))\n","\n","# Plotting the matrices with actual number values\n","fig, axes = plt.subplots(1, 1, figsize=(4, 6))\n","\n","# # Plot original small spectrogram with integer values\n","plot_matrix(small_spectrogram, 'Original Small Spectrogram (Integers)', axes, decimals=0)\n","\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's recreate the small matrix using only integer values for the unnormalized spectrogram.\n","small_spectrogram = np.random.randint(0, 100, (10, 5))\n","\n","# Perform min-max normalization across the frequency dimension (rows)\n","min_max_norm_freq_small = (small_spectrogram - small_spectrogram.min(axis=1)[:, None]) / \\\n","                          (small_spectrogram.max(axis=1) - small_spectrogram.min(axis=1))[:, None]\n","\n","# Perform min-max normalization across the time dimension (columns)\n","min_max_norm_time_small = (small_spectrogram - small_spectrogram.min(axis=0)) / \\\n","                          (small_spectrogram.max(axis=0) - small_spectrogram.min(axis=0))\n","\n","# Plotting the matrices with actual number values\n","fig, axes = plt.subplots(1, 2, figsize=(8, 6))\n","\n","# # Plot original small spectrogram with integer values\n","# plot_matrix(small_spectrogram, 'Original Small Spectrogram (Integers)', axes[0], decimals=0)\n","\n","# Plot min-max normalization across frequency with float values\n","plot_matrix(min_max_norm_freq_small, 'Min-Max Normalization\\n(Frequency)', axes[0])\n","\n","# Plot min-max normalization across time with float values\n","plot_matrix(min_max_norm_time_small, 'Min-Max Normalization\\n(Time)', axes[1])\n","\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import RobustScaler\n","\n","# Function to perform robust scaling across a specified axis\n","def robust_scale(matrix, axis):\n","    scaler = RobustScaler()\n","    if axis == 0:  # Scale each column independently\n","        scaled_matrix = scaler.fit_transform(matrix.T).T\n","    elif axis == 1:  # Scale each row independently\n","        scaled_matrix = scaler.fit_transform(matrix)\n","    return scaled_matrix\n","\n","# Apply robust scaling to the small_spectrogram across both axes\n","robust_scaled_freq_small = robust_scale(small_spectrogram, axis=1)  # Scale each row (frequency)\n","robust_scaled_time_small = robust_scale(small_spectrogram, axis=0)  # Scale each column (time)\n","\n","# Plotting the matrices with robust scaling applied\n","fig, axes = plt.subplots(1, 2, figsize=(8, 6))\n","\n","\n","\n","# Plot robust scaling across frequency with float values\n","plot_matrix(robust_scaled_freq_small, 'Robust Scaling\\n(Frequency)', axes[0])\n","\n","# Plot robust scaling across time with float values\n","plot_matrix(robust_scaled_time_small, 'Robust Scaling\\n(Time)', axes[1])\n","\n","plt.tight_layout()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"dl","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
