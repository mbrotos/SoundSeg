{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mbrotos/SoungSeg/blob/main/demo.ipynb)"]},{"cell_type":"markdown","metadata":{},"source":["# Singing Voice Separation by U-Net"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1702392495777,"user":{"displayName":"Adam Sorrenti","userId":"08015701831222800572"},"user_tz":300},"id":"2PuRNYOqfrNi","outputId":"cc5347ff-190d-4b9b-f3b0-0d063501661f"},"outputs":[],"source":["IN_COLAB = False\n","if 'google.colab' in str(get_ipython()):\n","    print('Running on CoLab, cloning repo, and installing requirements ...')\n","    IN_COLAB = True\n","    !git clone https://github.com/mbrotos/SoungSeg.git\n","    %cd SoungSeg/src\n","    !pip install -r requirements.txt\n","else:\n","    print('Not running on CoLab, creating local environment and installing requirements ...')\n","    %cd src\n","    !python -m venv venv\n","    !source ./venv/bin/activate\n","    !pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10817,"status":"ok","timestamp":1702392506591,"user":{"displayName":"Adam Sorrenti","userId":"08015701831222800572"},"user_tz":300},"id":"8OI3EtugfrNj"},"outputs":[],"source":["import os\n","import librosa\n","import IPython.display as ipd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from scipy import stats\n","import argparse\n","import config as cfg\n","import datetime\n","from scaler import normalize, denormalize\n","import pickle\n","import json\n","from augmentations import consecutive_oversample, blackout\n","from uuid import uuid4"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2107,"status":"ok","timestamp":1702392652696,"user":{"displayName":"Adam Sorrenti","userId":"08015701831222800572"},"user_tz":300},"id":"AM5uP9l-f42t","outputId":"b43a9912-d233-4678-e94d-832bf1573cdd"},"outputs":[],"source":["# Obtain dataset from Zenodo\n","# MUSDB18 - a corpus for music separation https://doi.org/10.5281/zenodo.3338373\n","# PLEASE NOTE: This dataset is quite large (>20GB -- zipped) and may take a while to download.\n","\n","# Alternatively, you can download a subset of the dataset (~9GB) with the following command: \n","# https://drive.google.com/file/d/1_kdifA4ztVXBveb9FYzmY49fvAKZmIJF/view?usp=sharing\n","!gdown 1_kdifA4ztVXBveb9FYzmY49fvAKZmIJF && unzip data_wav.zip && rm data_wav.zip\n","\n","# This cell may take 5 minutes to run. Please be patient."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create required directories\n","!mkdir -p models processed_data\n","\n","# Preprocess dataset\n","!python preprocessing.py --dsType train\n","!python preprocessing.py --dsType test\n","\n","# Tensorflow dataset prep\n","!python dataset_prep.py"]},{"cell_type":"markdown","metadata":{},"source":["#### Song mixutre and vocal example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"536YBpl_iYZS"},"outputs":[],"source":["mix_mags_train = np.load(\"./processed_data/mix_mags_train_512x128.npy\", mmap_mode='r' )\n","mix_phases_train = np.load(\"./processed_data/mix_phases_train_512x128.npy\", mmap_mode='r')\n","vocal_train = np.load(f\"./processed_data/vocal_mags_train_512x128.npy\",mmap_mode='r')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c80cu85NfrNl"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### Visualize the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQoPH_iQfrNl"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Run unit tests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ex7NzjQZfrNl"},"outputs":[],"source":["!python test_audio_processing.py"]},{"cell_type":"markdown","metadata":{},"source":["### Define Model Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0DgJpmGfrNm"},"outputs":[],"source":["import tensorflow as tf\n","from keras.layers import Activation, Conv2D, BatchNormalization, Conv2DTranspose, Concatenate, MaxPooling2D, Input, Conv1D, Normalization\n","\n","def get_model(img_size, num_classes=1):\n","    inputs = Input(shape=img_size + (1,))\n","\n","    conv1 = Conv2D(64, 3, strides=1, padding=\"same\")(inputs)\n","    conv1 = BatchNormalization()(conv1)\n","    conv1 = Activation(\"relu\")(conv1)\n","\n","    conv2 = Conv2D(64, 3, strides=1, padding=\"same\")(conv1)\n","    conv2 = BatchNormalization()(conv2)\n","    conv2 = Activation(\"relu\")(conv2)\n","\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","\n","    conv3 = Conv2D(128, 3, strides=1, padding=\"same\")(pool1)\n","    conv3 = BatchNormalization()(conv3)\n","    conv3 = Activation(\"relu\")(conv3)\n","\n","    conv4 = Conv2D(128, 3, strides=1, padding=\"same\")(conv3)\n","    conv4 = BatchNormalization()(conv4)\n","    conv4 = Activation(\"relu\")(conv4)\n","\n","    pool2 = MaxPooling2D(pool_size=(2, 2))(conv4)\n","\n","    conv5 = Conv2D(256, 3, strides=1, padding=\"same\")(pool2)\n","    conv5 = BatchNormalization()(conv5)\n","    conv5 = Activation(\"relu\")(conv5)\n","\n","    conv6 = Conv2D(256, 3, strides=1, padding=\"same\")(conv5)\n","    conv6 = BatchNormalization()(conv6)\n","    conv6 = Activation(\"relu\")(conv6)\n","\n","    pool3 = MaxPooling2D(pool_size=(2, 2))(conv6)\n","\n","    conv7 = Conv2D(512, 3, strides=1, padding=\"same\")(pool3)\n","    conv7 = BatchNormalization()(conv7)\n","    conv7 = Activation(\"relu\")(conv7)\n","\n","    conv8 = Conv2D(512, 3, strides=1, padding=\"same\")(conv7)\n","    conv8 = BatchNormalization()(conv8)\n","    conv8 = Activation(\"relu\")(conv8)\n","\n","    pool4 = MaxPooling2D(pool_size=(2, 2))(conv8)\n","\n","    conv9 = Conv2D(1024, 3, strides=1, padding=\"same\")(pool4)\n","    conv9 = BatchNormalization()(conv9)\n","    conv9 = Activation(\"relu\")(conv9)\n","\n","    conv10 = Conv2D(1024, 3, strides=1, padding=\"same\")(conv9)\n","    conv10 = BatchNormalization()(conv10)\n","    conv10 = Activation(\"relu\")(conv10)\n","\n","    up1 = Conv2DTranspose(512, 2, strides=2, padding=\"same\")(conv10)\n","    up1 = Concatenate()([up1, conv8])\n","\n","    upconv1 = Conv2D(512, 3, strides=1, padding=\"same\")(up1)\n","    upconv1 = BatchNormalization()(upconv1)\n","    upconv1 = Activation(\"relu\")(upconv1)\n","\n","    upconv2 = Conv2D(512, 3, strides=1, padding=\"same\")(upconv1)\n","    upconv2 = BatchNormalization()(upconv2)\n","    upconv2 = Activation(\"relu\")(upconv2)\n","\n","    up2 = Conv2DTranspose(256, 2, strides=2, padding=\"same\")(upconv2)\n","    up2 = Concatenate()([up2, conv6])\n","\n","    upconv3 = Conv2D(256, 3, strides=1, padding=\"same\")(up2)\n","    upconv3 = BatchNormalization()(upconv3)\n","    upconv3 = Activation(\"relu\")(upconv3)\n","\n","    upconv4 = Conv2D(256, 3, strides=1, padding=\"same\")(upconv3)\n","    upconv4 = BatchNormalization()(upconv4)\n","    upconv4 = Activation(\"relu\")(upconv4)\n","\n","    up3 = Conv2DTranspose(128, 2, strides=2, padding=\"same\")(upconv4)\n","    up3 = Concatenate()([up3, conv4])\n","\n","    upconv5 = Conv2D(128, 3, strides=1, padding=\"same\")(up3)\n","    upconv5 = BatchNormalization()(upconv5)\n","    upconv5 = Activation(\"relu\")(upconv5)\n","\n","    upconv6 = Conv2D(128, 3, strides=1, padding=\"same\")(upconv5)\n","    upconv6 = BatchNormalization()(upconv6)\n","    upconv6 = Activation(\"relu\")(upconv6)\n","\n","    up4 = Conv2DTranspose(64, 2, strides=2, padding=\"same\")(upconv6)\n","    up4 = Concatenate()([up4, conv2])\n","\n","    upconv7 = Conv2D(64, 3, strides=1, padding=\"same\")(up4)\n","    upconv7 = BatchNormalization()(upconv7)\n","    upconv7 = Activation(\"relu\")(upconv7)\n","\n","    upconv8 = Conv2D(64, 3, strides=1, padding=\"same\")(upconv7)\n","    upconv8 = BatchNormalization()(upconv8)\n","    upconv8 = Activation(\"relu\")(upconv8)\n","\n","    output = Conv1D(num_classes, 1, activation=\"linear\")(upconv8)\n","\n","    # Define the model\n","    model = tf.keras.Model(inputs, output)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--epochs\", type=int, default=10, help='Number of epochs to train for')\n","parser.add_argument(\"--batch_size\", type=int, default=5, help='Batch size for training')\n","parser.add_argument(\"--normalization\", type=str, default=\"frequency\", help='Normalization axis (time or frequency)')\n","parser.add_argument(\"--lr\", type=float, default=1e-3, help='Learning rate for training')\n","parser.add_argument(\"--mask\", action=\"store_true\", default=False, help='Experimental. Causes unstable training.')\n","parser.add_argument(\"--quantile_scaler\", action=\"store_true\", default=False, help='Toggle quantile scaling as the normalization method')\n","parser.add_argument(\"--q_min\", type=float, default=25.0, help='Minimum quantile for quantile scaling')\n","parser.add_argument(\"--q_max\", type=float, default=75.0, help='Maximum quantile for quantile scaling')\n","parser.add_argument(\"--loss\", type=str, default=\"mse\", help='Loss function to use (mse or mae)')\n","parser.add_argument(\"--dataset_size\", type=int, default=None, help='Number of samples to use from the dataset (None = all)')\n","parser.add_argument(\"--augmentations\", action=\"store_true\", default=False, help='Toggle data augmentations (splicing, and blackout)')\n","parser.add_argument(\"--seed\", type=int, default=42, help='Random seed for reproducibility')\n","parser.add_argument(\"--mmap\", action=\"store_true\", default=True, help='Toggle memory mapping for dataset loading (helps with large datasets and limited RAM)')\n","\n","# Top performaning model args, the batch size has been reduced from 64 to 25 to fit on colab GPUs\n","args = parser.parse_args(['--normalization', 'frequency', '--epochs', '20', '--batch_size', '25', '--loss', 'mae', '--augmentations'])\n","\n","print('Args:')\n","print(args)\n","\n","np.random.seed(args.seed)\n","tf.random.set_seed(args.seed)"]},{"cell_type":"markdown","metadata":{},"source":["### Define datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tliCGkG0frNm"},"outputs":[],"source":["# load data\n","mix_mags_train = np.load(\"./processed_data/mix_mags_train_512x128.npy\", mmap_mode='r' if args.mmap else None)[:args.dataset_size]\n","mix_phases_train = np.load(\"./processed_data/mix_phases_train_512x128.npy\", mmap_mode='r' if args.mmap else None)[:args.dataset_size]\n","vocal_train = np.load( f\"./processed_data/vocal_mags_train_512x128.npy\", mmap_mode='r' if args.mmap else None)[:args.dataset_size]\n","\n","mix_mags_train_norm, vocal_train_norm, mix_mags_train_norm_factors = normalize(\n","    np.copy(mix_mags_train),\n","    np.copy(vocal_train),\n","    normalization=args.normalization,\n","    quantile_scaler=args.quantile_scaler,\n","    q_min=args.q_min,\n","    q_max=args.q_max,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if args.augmentations:\n","    print('Appling augmentations...')\n","    \n","    # Remove outliers\n","    true_vocal = denormalize(\n","        vocal_train_norm,\n","        mix_mags_train_norm_factors,\n","        normalization=args.normalization,\n","        quantile_scaler=args.quantile_scaler,\n","    )\n","\n","    vocal_waves = []\n","\n","    for i in range(0, len(true_vocal)):\n","        cur_phase = np.concatenate(mix_phases_train[i : i + 1], axis=1)\n","        cur_true_vocal = np.concatenate(true_vocal[i : i + 1], axis=1)\n","        vocal_waves.append(librosa.istft(\n","                cur_true_vocal[:, :, 0] * cur_phase[:, :, 0],\n","                hop_length=cfg.HOP_SIZE,\n","                window=\"hann\",\n","            )\n","        )\n","    vocal_waves = np.array(vocal_waves)\n","    dist = np.abs(vocal_waves).sum(axis=1)\n","    indices = np.where(dist < 100)[0]\n","\n","    mix_mags_train_norm = np.delete(mix_mags_train_norm, indices, axis=0)\n","    vocal_train_norm = np.delete(vocal_train_norm, indices, axis=0)\n","    mix_mags_train_norm_factors = np.delete(mix_mags_train_norm_factors, indices, axis=0)\n","    mix_phases_train = np.delete(mix_phases_train, indices, axis=0)\n","    \n","    # Splicing and blackout\n","    mix_blackout, vocal_blackout = blackout(mix_mags_train_norm, vocal_train_norm)\n","    mix_blackout = mix_blackout[:mix_blackout.shape[0]//4]\n","    vocal_blackout = vocal_blackout[:vocal_blackout.shape[0]//4]\n","    mix_consec, vocal_consec = consecutive_oversample(mix_mags_train_norm, vocal_train_norm)\n","    mix_consec = mix_consec[:mix_consec.shape[0]//2]\n","    vocal_consec = vocal_consec[:vocal_consec.shape[0]//2]\n","\n","    mix_mags_train_norm = np.concatenate((mix_mags_train_norm, mix_consec, mix_blackout), axis=0)\n","    vocal_train_norm = np.concatenate((vocal_train_norm, vocal_consec, vocal_blackout), axis=0)\n","    \n","    \n","    \n","    \n","print('Datasets:')\n","print(f'Mixes: {mix_mags_train_norm.shape}')\n","print(f'Vocals: {vocal_train_norm.shape}')"]},{"cell_type":"markdown","metadata":{},"source":["#### Augmentation Visualizations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_spectrogram_with_shading(spectrogram, title, subplot_index, shade_start=None, shade_end=None):\n","    plt.subplot(subplot_index)\n","    plt.imshow(spectrogram, aspect='auto', origin='lower')\n","    if shade_start is not None and shade_end is not None:\n","        plt.axvspan(shade_start, shade_end, color='red', alpha=0.2)\n","    plt.title(title)\n","    plt.colorbar()\n","\n","i=0\n","\n","# Plotting\n","plt.figure(figsize=(12, 8))\n","\n","# Original Mix Spectrogram\n","plot_spectrogram_with_shading(mix_mags_train_norm[i], 'Original Mix', 321)\n","\n","# Original Vocal Spectrogram\n","plot_spectrogram_with_shading(vocal_train_norm[i], 'Original Vocal', 322)\n","\n","# Blackout Mix Spectrogram\n","plot_spectrogram_with_shading(mix_blackout[i], 'Blackout Mix', 323)\n","\n","# Blackout Vocal Spectrogram\n","plot_spectrogram_with_shading(vocal_blackout[i], 'Blackout Vocal', 324)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Plotting Consecutive Oversample with shaded highlights\n","plt.figure(figsize=(18, 6))\n","\n","# Original Mix Spectrogram with shading on the last half\n","plot_spectrogram_with_shading(mix_mags_train_norm[i], 'Original Mix', 131, shade_start=64, shade_end=128)\n","\n","# Original Mix + 1 Spectrogram with shading on the first half\n","plot_spectrogram_with_shading(mix_mags_train_norm[i+1], 'Original Mix + 1', 132, shade_start=0, shade_end=64)\n","\n","# Consecutive Oversample Mix Spectrogram with corresponding shaded highlights\n","plot_spectrogram_with_shading(mix_consec[i], 'Consecutive Oversample Mix', 133)\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","model_name = f\"model_{timestamp}_{uuid4().hex}\"\n","    \n","# Save normalization factors as pkl\n","with open(f\"./models/scaler--{model_name}.pkl\", \"wb\") as f:\n","    pickle.dump(mix_mags_train_norm_factors, f)\n","\n","data_len = len(mix_mags_train_norm)\n","\n","# shuffle datasets before splitting\n","indices = np.arange(data_len)\n","np.random.shuffle(indices)\n","mix_mags_train_norm = mix_mags_train_norm[indices]\n","vocal_train_norm = vocal_train_norm[indices]\n","\n","val_len = int(data_len * 0.1)\n","val_data = (\n","    mix_mags_train_norm[-val_len:],\n","    vocal_train_norm[-val_len:],\n",")\n","train_data = (\n","    mix_mags_train_norm[:-val_len],\n","    vocal_train_norm[:-val_len],\n",")\n","\n","dataset = tf.data.Dataset.from_tensor_slices(train_data)\n","dataset = (\n","    dataset.shuffle(args.batch_size * 2)\n","    .batch(args.batch_size)\n","    .prefetch(tf.data.AUTOTUNE)\n",")\n","val_ds = tf.data.Dataset.from_tensor_slices(val_data)\n","val_ds = val_ds.batch(args.batch_size).prefetch(tf.data.AUTOTUNE)"]},{"cell_type":"markdown","metadata":{},"source":["### Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = get_model((cfg.FREQUENCY_BINS, cfg.SAMPLE_SZ), num_classes=1)\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(args.lr),\n","    loss=args.loss,\n",")\n","\n","os.makedirs(f\"./models/{model_name}\")\n","os.makedirs(f\"./models/{model_name}/logs\")\n","\n","# Save args as json\n","with open(f\"./models/{model_name}/args.json\", \"w\") as f:\n","    json.dump(vars(args), f, indent=4)\n","    \n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwOW4PaRfrNm"},"outputs":[],"source":["history = model.fit(\n","    dataset,\n","    validation_data=val_ds,\n","    epochs=args.epochs,\n","    callbacks=[\n","        tf.keras.callbacks.ModelCheckpoint(\n","            filepath=f\"./models/{model_name}/{model_name}-val.hdf5\", save_best_only=True, monitor='val_loss', mode='min', save_weights_only=True\n","        ),\n","        tf.keras.callbacks.ModelCheckpoint(\n","            filepath=f\"./models/{model_name}/{model_name}-train.hdf5\", save_best_only=True, monitor='loss', mode='min', save_weights_only=True\n","        ),\n","        tf.keras.callbacks.TensorBoard(\n","            log_dir=f\"./models/{model_name}/logs\", histogram_freq=1\n","        ),\n","    ],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","# Extracting the number of epochs\n","epochs = range(1, len(loss) + 1)\n","\n","# Plotting the loss and validation loss\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs, loss, 'bo-', label='Training loss')\n","plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluate the model"]},{"cell_type":"markdown","metadata":{},"source":["#### Test voice seperation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9NHtFDifrNn"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### MIR_Eval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZBaoPqXfrNn"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Comparing data normalization techniques"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"98DcsBTcfrNn"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"dl","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
